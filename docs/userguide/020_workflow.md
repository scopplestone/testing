# Workflow

In this chapter, the complete process of setting up a simulation with **PICLas** is detailed.

## Mesh generation/conversion with HOPR

**PICLas** utilizes computational meshes from the high order preprocessor **HOPR** (available under GPLv3 at [https://www.hopr-project.org](https://www.hopr-project.org)) in HDF5 format. The design philosophy is that all tasks related to mesh organization, different input formats and the construction of high order geometrical mappings are separated from the *parallel* simulation code. These tasks are implemented most efficiently in a *serial* environment. The employed mesh format is designed to make the parallel read-in process as simple and fast as possible. For details concerning the mesh format please refer to the [HOPR HDF5 Curved Mesh Format Documentation](https://www.hopr-project.org/upload/e/e6/MeshFormat.pdf). Installation instructions can be found [here](https://www.hopr-project.org/index.php/Installation).

Using **HOPR**, simple, structured meshes can be directly created using an [in-built mesh generator.](https://www.hopr-project.org/index.php/Inbuilt_Mesh_Generators) A number of strategies to create curved boundaries are also included in HOPR. More complex geometries can be treated by importing meshes generated by external mesh generators in CGNS or GMSH format ([Example parameter file](https://www.hopr-project.org/index.php/External_Meshes)). Detailed instructions for some mesh generators that we have experience with (GridPro, HEXPRESS, MeshGems within SALOME and CENTAUR) are given below.

The basic command for either mesh generation or conversion of an external mesh is

~~~~~~~
hopr hopr.ini
~~~~~~~

Note that the path to the **HOPR** executable is omitted in the command (visit \ref{sec:installation_directory}).

### Mesh generation with HEXPRESS

#### Hexpress Setup
Download HEXPRESS from the official website and install the program. Execute the configure script under
```
/usr/numeca/COMMON/configure
```

Add the licence server (e.g. to *.bashrc*)
```
NUMECA_LICENSE_FILE=@name_server
export NUMECA_LICENSE_FILE
```
where *name_servers* is the server alias or address.
Next, adjust the *.driver * file in the *.numeca* directory.
```
~/.numeca/.driver
```
with the following line
```
name_server localhost OPENGL:OPENGL2:X11 X11
```

#### Hexpress Usage
CAD model of complete fluid domain with FreeCAD -> Export as STL. CATIA, SolidWorks formats also supported by HEXPRESS.

Export as CGNS (ADF)

HOPR: CGNS 3.3.1

### Mesh generation with GridPro

[GridPro](https://www.gridpro.com/) is a proprietary conforming multi-block mesh generator with hexahedral elements. However, a free academic version limited to 250 blocks is available.

After mesh generation, and before naming the boundaries in the *Property Setter*, you should set the output format to STARCD. Make sure to define not only labels but also different properties for the boundaries. Then export as STARCD and you will get four output files. During the export GridPro loses the label information, thus the boundary names have to be set again in the *.inp file. An example of a correct *.inp is given below:

    TITLE
    Converted from GridPro v4.1
    CTAB 1 FLUI
    CTNA 1 ZONE_VOL
    RDEF 1 WALL $ $ $ $ $ $ $ $
    RNAM 1 BC_WALL
    RDEF 2 CYCL $ $ $ $ $ $ $ $
    RNAM 2 BC_CYCL
    VREAD,/home/user/mesh/test.vrt,,1,43770,CODE
    CREAD,/home/user/mesh/test.cel,,1,21504,MODI,CODE
    BREAD,/home/user/mesh/test.bnd,,1,43768,MODI,CODE

HOPR can then read-in the mesh with following mode option:

    Mode = 4

More recent versions of GridPro also support a CGNS output. Here, the option *Export* -> *Grid* -> *CGNS* -> *Elementary* should be chosen. For different boundary labels, different property types have to be defined (Note: The property type *Wall* might be causing problems during the HOPR read-in and should be avoided). The following errors can be ignored as long as HOPR finishes successfully and a mesh file is written out

    ERROR: number of zones in inifile does not correspond to number of zones in meshfile(s)
    ERROR - Could not find corresponding boundary definition of ws.Interblck

### Mesh generation with CENTAUR

A conventional tetrahedral mesh can be generated with CENTAUR. Boundaries have to be set in CENTAUR and accordingly in the HOPR parameter file. During the export as CGNS the following options are required:

* Check "Only write out boundary faces"
* Check "Write out boundary faces grouped by panels"

Read-in and convert with HOPR and the following options:

    Mode = 3
    BugFix_ANSA_CGNS = TRUE
    SplitToHex = TRUE

Should problems occur try to set SpaceQuandt to a higher value, e.g. 100. During the pre-processing step every tetrahedron will be converted to 4 hexahedrons, resulting in increased number of elements.

### Mesh generation with MeshGems/SALOME

*Note: This tutorial was last updated/used: June 2016*

[MeshGems-Hexa](http://www.meshgems.com/volume-meshing-meshgems-hexa.html) is proprietary automated all-hex mesh generator. The algorithm can be used a plug-in within the open-source platform [SALOME](https://www.salome-platform.org/). At the time, we do not have a MeshGems-Hexa license and cannot give any support on mesh generation.

* Import geometry as STEP or IGS
* Create groups in Geometry module for the boundary conditions
  * Right-click on the imported file in Object Browser -> "Create Group"
  * Select faces as "Share Type" (2D object)
  * Choose appropriate name for the BC
  * Select one or many surfaces by (shift-) clicking on the surfaces
  * Click "Add" when finished for a single BC
  * "Apply" to save the BC and repeat the process (or "Apply and Close" at the end)
* Create mesh in Mesh module: "Mesh" -> "Create mesh"
    * Choose the imported geometry in "Geometry" (if not already chosen, when geometry was highlighted previously)
    * Choose MG-Hexa for the 3D algorithm
    * Choose MG-CADsurf for the 2D algorithm
* Assign boundary conditions to mesh
    * Right-click on created mesh
    * "Create groups from Geometry"
      * Click on the BC in "Object Browser" in the "Geometry"-tree
      * "Apply" and repeat with the next BC
      * "Apply and close" with the last BC
* Export mesh as CGNS
    * Right-click on mesh, choose "Export" -> "CGNS file"
    * Import the exported CGNS file
    * Check the names of boundary conditions
      * Activate the BC by clicking on the visibility icon next to the name
      * Change the name to correspond to the entry in the preproc.ini
    * Export the modified mesh again as CGNS
* Convert CGNS mesh from hdf to adf format
        Install cgns-tools ("sudo apt-get install cgns-tools")
        "hdf2adf filename"

Read-in and convert with HOPR and the following options:

    Mode = 3
    BugFix_ANSA_CGNS = TRUE

(sec:compiler-options)=
## Compiler options
This section describes the main configuration options which can be set when building **PICLas** using CMake.
Some options are dependent on others being enabled (or disabled), so the available ones may change.

The first set of options describe general CMake behaviour:

* ``CMAKE_BUILD_TYPE``: This statically specifies what build type (configuration) will be built in this build tree. Possible values are
    * Release: "Normal" execution.
    * Profile: Performance profiling using gprof.
    * Debug: Debug compiler for detailed error messages during code development.
    * SANI: Sanitizer compiler for even more detailed error messages during code development.

* ``CMAKE_HOSTNAME``: This will display the host name of the machine you are compiling on.

* ``CMAKE_INSTALL_PREFIX``: If “make install” is invoked or INSTALL is built, this directory is prepended onto all install directories. This variable defaults to /usr/local on UNIX.

For some external libraries and programs that **PICLas** uses, the following options apply:

* ``CTAGS_PATH``: This variable specifies the Ctags install directory, an optional program used to jump between tags in the source file.

* ``LIBS_BUILD_HDF5``: This will be set to ON if no prebuilt HDF5 installation was found on your machine. In this case a HDF5 version will be build and used instead. For a detailed description of the installation of HDF5, please refer to Section \ref{sec:install_hdf5}.

* ``HDF5_DIR``: If you want to use a prebuilt HDF5 library that has been build using the CMake system, this directory should contain the CMake configuration file for HDF5 (optional).

* ``PICLAS_BUILD_POSTI``: Enables the compilation of additional tools and activates the following options:
  * ``POSTI_BUILD_SUPERB``: Enables the compilation of **superB**, which is allows the computation of magnetic fields based on an input of coils and permanent magnets (Section \ref{sec:superB})
  * ``POSTI_BUILD_VISU``: Enables the compilation of the post-processing tool **piclas2vtk**, which enables the conversion of output files into the VTK format (Chapter \ref{chap:visu_output})
  * ``POSTI_USE_PARAVIEW``: Enables the compilation of the ParaView plugin, which enables the direct read-in of output files within ParaView

## Solver settings

Before setting up a simulation, the code must be compiled with the desired parameters. The most important compiler options to be set are:

* ``PICLAS_TIMEDISCMETHOD``: Module selection
    * DSMC: Direct Simulation Monte Carlo
    * RK4: Time integration method Runge-Kutta 4th order in time
* ``PICLAS_EQNSYSNAME``: Equation system to be solved
    * maxwell:
    * poisson:
* ``PICLAS_POLYNOMIAL_DEGREE``: Defines the polynomial degree of the solution. The order of convergence follows as $N+1$. Each grid cell contains $(N+1)^3$ collocation points to represent the solution.
* ``PICLAS_NODETYPE``: The nodal collocation points used during the simulation
    * GAUSS:
    * GAUSS-LOBATTO:
* ``PICLAS_INTKIND8``: Enables simulations with particle numbers above 2 147 483 647
* ``PICLAS_READIN_CONSTANTS``: Enables user-defined natural constants for the speed of light *c0*, permittivity *eps* and permeability *mu* of vacuum,
    which must then be supplied in the parameter file. The default if *OFF* and the values for the speed of light c0=299792458.0 [m/s], permittivity
    eps=8.8541878176e-12 [F/m] and permeability mu=1.2566370614e-6 [H/m] are hard-coded.

The options EQNSYSNAME, POLYNOMIAL_DEGREE and NODETYPE can be ignored for a DSMC simulation. For parallel computation the following flags should be configured:

* ``LIBS_USE_MPI``: Enabling parallel computation. For a detailed description of the installation of MPI, please refer to refer to Section \ref{sec:install_mpi}.
* ``PICLAS_LOADBALANCE``: Enable load-balancing

All other options are set in the parameter file.

## Setup of parameter file(s)

The settings of the simulation are controlled through parameter files, which are given as arguments to the binary. In the case of PIC simulations the input of a single
parameter file (e.g. *parameter.ini*) is sufficient, while the DSMC method requires the input of a species parameter file (e.g. *DSMCSpecies.ini*). The most recent list of parameters can be found by invoking the help in the console:

    piclas --help

General parameters such the name of project (used for filenames), the mesh file (as produced by HOPR), end time of the simulation (in seconds) and the time step, at which the particle data is written out (in seconds), are:

    ProjectName    = TestCase
    MeshFile       = test_mesh.h5
    TEnd           = 1e-3
    Analyze_dt     = 1e-4
    ManualTimeStep = 1e-4 (over-rides the automatic time step calculation in the Maxwell solver)

Generally following types are used:

~~~~~~~
INTEGER = 1
REAL    = 1.23456
REAL    = 1.23E12
LOGICAL = T         ! True
LOGICAL = F         ! False
STRING  = PICLAS
VECTOR  = (/1.0,2.0,3.0/)
~~~~~~~

The concept of the parameter file is described as followed:

* Each single line is saved and examined for specific variable names
* The examination is case-insensitive
* Comments can be set with symbol "!" in front of the text
* Numbers can also be set by using "pi"
~~~~~~~
    vector = (/1,2Pi,3Pi/)
~~~~~~~
* The order of defined variables is with one exception irrelevant, except for the special case when redefining boundaries. However, it is preferable to group similar variables together.

The options and underlying models are discussed in Chapter \ref{chap:features_models}, while the available output options are given in Chapter \ref{chap:visu_output}. Due to the sheer number of parameters available, it is advisable to build upon an existing parameter file from one of the tutorials in Chapter \ref{chap:tutorials}.

## Simulation

After the mesh generation, compilation of the binary and setup of the parameter files, the code can be executed by

    piclas parameter.ini [DSMCSpecies.ini]

The simulation may be restarted from an existing state file

    piclas parameter.ini [DSMCSpecies.ini] [restart_file.h5]

A state file is generated at the end of the simulation and also at every time step defined by `Analyze_dt`. **Note:** When restarting from an earlier time (or zero), all later state files possibly contained in your directory are deleted!

After a successful simulation, state files will be written out in the HDF5 format preceded by the project name, file type (e.g. State, DSMCState, DSMCSurfState) and the time stamp:

    TestCase_State_001.5000000000000000.h5
    TestCase_DSMCState_001.5000000000000000.h5

The format and floating point length of the time stamp *001.5000000000000000* can be adjusted with the parameter

    TimeStampLength = 21

where the floating format with length of *F21.14* is used as default value.

### Parallel execution
The simulation code is specifically designed for (massively) parallel execution using the MPI library. For parallel runs, the code must be compiled with `PICLAS_MPI=ON`. Parallel execution is then controlled using `mpirun`

    mpirun -np [no. processors] piclas parameter.ini [DSMCSpecies.ini] [restart_file.h5]

The grid elements are organized along a space-filling curved, which gives a unique one-dimensional element list. In a parallel run, the mesh is simply divided into parts along the space filling curve. Thus, domain decomposition is done *fully automatic* and is not limited by e.g. an integer factor between the number of cores and elements. The only limitation is that the number of cores may not exceed the number of elements.

## Post-processing

**PICLas** comes with a tool for visualization. The piclas2vtk tool converts the HDF5 files generated by **PICLas** to the binary VTK format, readable by many visualization tools like ParaView and VisIt. The tool is executed by

~~~~~~~
piclas2vtk [posti.ini] output.h5
~~~~~~~

Multiple HDF5 files can be passed to the piclas2vtk tool at once. The (optional) runtime parameters to be set in `posti.ini` are given in Chapter \ref{chap:visu_output}.
